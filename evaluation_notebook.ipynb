{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /home/brandon/.local/lib/python3.8/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import re\n",
    "import os\n",
    "from spacy import displacy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "FOODKEEPER_PATH = \"datasets/FoodKeeper-Data.xls\"\n",
    "TRAINING_DATA_PATH = \"datasets/data.csv\"\n",
    "MODEL_PATH = \"output/model-last\"\n",
    "TEST_DATA_PATH = \"datasets/test_data.csv\"\n",
    "\n",
    "#STARTING_KEYWORD_COUNT = 10\n",
    "#TRAINING_LOOP_ITERATIONS = 3\n",
    "#REQUIRED_KEYWORDS = 3\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "nlp = spacy.load(MODEL_PATH)\n",
    "food_data = pd.read_excel(FOODKEEPER_PATH, sheet_name = \"Product\")\n",
    "training_data = pd.read_csv(TRAINING_DATA_PATH,index_col = False, header = None)\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "#write a method to find the top x keywords in the dataset\n",
    "#loop through and count the specific entities\n",
    "keywords = [] #'chicken', 'milk', 'butter', 'cheese'\n",
    "sampleData = []\n",
    "\n",
    "\n",
    "    \n",
    "#update rank tweet to take the counter as a parameter and condense both rankings\n",
    "def rankTweet(tweet):\n",
    "    tweetKeywords = []\n",
    "    doc = nlp(tweet)\n",
    "    return len(doc.ents)\n",
    "       \n",
    "        \n",
    "    print(tweetKeywords)\n",
    "    \n",
    "def findNewKeywords(tweet, keywords):\n",
    "    foodkeeperKeys = foodKeeperInfo()\n",
    "    x = tweet.split()\n",
    "    word = \"\"\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "    #for i in range(len(x)):\n",
    "        z = 1\n",
    "        if x[i] in foodkeeperKeys:\n",
    "            word = x[i]\n",
    "        try:\n",
    "            foundBiWord = x[i] + \" \" + x[i+1]\n",
    "            if foundBiWord in keywords:\n",
    "                word = foundBiWord\n",
    "                z = 2\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            foundTriWord = x[i] + \" \" + x[i+1] + \" \" + x[i+2]\n",
    "            if foundTriWord in keywords:\n",
    "                word = foundTriWord\n",
    "                z = 3\n",
    "        except:\n",
    "            pass\n",
    "        i += z\n",
    "        \n",
    "        if word not in keywords and word != \"\":\n",
    "            keywords.append(word)\n",
    "    #print(keywords)\n",
    "    \n",
    "#def findInitialKeywords(data):\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "keywordRanker = {}   \n",
    "\n",
    "def convertToTrainingFormat(tweet, keywords):\n",
    "    \n",
    "    \n",
    "    x = tweet.split()\n",
    "    myEnts = {'entities':[]}\n",
    "    found = False\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        z = 1\n",
    "        newWord = \"\"\n",
    "        if x[i] in keywords:\n",
    "            pos = tweet.find(x[i])\n",
    "            y = (pos, pos + len(x[i]), 'FOOD')\n",
    "            found = True\n",
    "        if x[i] in foodKeeperKeywordsTest:\n",
    "            newWord = x[i]\n",
    "            \n",
    "        try:\n",
    "            foundBiWord = x[i] + \" \" + x[i+1]\n",
    "            if foundBiWord in keywords:\n",
    "                pos = tweet.find(x[i])\n",
    "                y = (pos, pos + len(x[i])+len(x[i+1]) + 1, 'FOOD')\n",
    "                found = True\n",
    "                z = 2\n",
    "            if foundBiWord in foodKeeperKeywordsTest:\n",
    "                newWord = foundBiWord\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            foundTriWord = x[i] + \" \" + x[i+1] + \" \" + x[i+2]\n",
    "            if foundTriWord in keywords:\n",
    "                pos = tweet.find(x[i])\n",
    "                y = (pos, pos + len(x[i])+len(x[i+1])+len(x[i+2]) + 2, 'FOOD')\n",
    "                found = True\n",
    "                z = 3\n",
    "            if foundTriWord in foodKeeperKeywordsTest:\n",
    "                newWord = foundTriWord\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if y not in myEnts['entities']:\n",
    "                myEnts['entities'].append(y) \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        if newWord != \"\" and newWord not in keywordRanker:\n",
    "            keywordRanker[newWord] = 1\n",
    "        elif newWord != \"\" and newWord in keywordRanker:\n",
    "            keywordRanker[newWord] += 1\n",
    "        #print(z)\n",
    "        i += z\n",
    "        #print(i)\n",
    "        \n",
    "        \n",
    "    formatted = (tweet, myEnts)\n",
    "    #print(formatted)\n",
    "    if found:\n",
    "        return formatted\n",
    "    else: return ()\n",
    "   \n",
    "    \n",
    "\n",
    "#print(convertToTrainingFormat('I like chicken and salsa', keywords))\n",
    "\n",
    "#print(food_data['Keywords'])\n",
    "\n",
    "def foodKeeperInfo():              \n",
    "    keywords = []\n",
    "    for word in food_data['Name']:\n",
    "        word = word.lstrip()\n",
    "        word = word.rstrip()\n",
    "\n",
    "        if word.lower() not in keywords: \n",
    "            keywords.append(word.lower())\n",
    "\n",
    "    #print(\"Total foodkeeper food names: \" + str(len(keywords)))        \n",
    "    #for element in sorted(keywords):\n",
    "        #print(element)\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "foodKeeperKeywordsTest = foodKeeperInfo()\n",
    "\n",
    "\n",
    "def preProcess(tweet):\n",
    "    #Converts a tweet to lowercase, replaces anyusername w/ <USERNAME> and URLS with <URL>\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('@[a-zA-z0-9]*', '<USERNAME>', tweet)\n",
    "    tweet = re.sub('https[a-zA-z0-9./:]*', '<URL>', tweet)\n",
    "    tweet = re.sub('[.,-]*', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "noEntity= []\n",
    "               \n",
    "           \n",
    "def trainModel(data):\n",
    "    counter = 0\n",
    "    while counter < 3:\n",
    "        print(\"~~~~~~~~~~~~~~~~~\"+str(counter)+\"~~~~~~~~~~~~~~~~~\")\n",
    "        nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "        db = DocBin() # create a DocBin object\n",
    "\n",
    "        myTweets = []\n",
    "        #Loop through all the tweets\n",
    "        #print(keywordRanker)\n",
    "        for i in range(len(data[0])):\n",
    "            \n",
    "            if counter == 1:\n",
    "                x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                if x!= ():\n",
    "                    if len(x[1]['entities']) > 3:\n",
    "                        #print(\"Found tweet\", x[0])\n",
    "                        \n",
    "                        myTweets.append(x)  \n",
    "        \n",
    "            else:\n",
    "                #convert each tweet into spacy training format\n",
    "                x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                if x != ():\n",
    "                    #print(len(x[1]['entities'])\n",
    "                    #check the ranking of the tweet\n",
    "                    if rankTweet(x[0]) > 2:\n",
    "                        #print(\"Checking rank...\")\n",
    "                        myTweets.append(x)\n",
    "\n",
    "                        \n",
    "        #Initialize the keywords\n",
    "        #print(\"\\n\\n\\nKeyword Ranker\\n\")\n",
    "        sortedKeywords =  sorted(keywordRanker, key=keywordRanker.get, reverse=True)\n",
    "        \n",
    "        #Chooses first 10 keywords to be used as basis for Snowball\n",
    "        if counter == 0: \n",
    "            for i in range(10):\n",
    "                keywords.append(sortedKeywords[i])\n",
    "            #print(sortedKeywords[i], keywordRanker[sortedKeywords[i]])\n",
    "                \n",
    "        if counter > 0:\n",
    "            for text, annot in tqdm(myTweets): # data in previous format\n",
    "                doc = nlp.make_doc(text) # create doc object from text\n",
    "                ents = []\n",
    "                for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "                    span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "                    if span is None:\n",
    "                        print(\"Skipping entity\")\n",
    "                    else:\n",
    "                        ents.append(span)\n",
    "                doc.ents = ents # label the text with the ents\n",
    "                db.add(doc)\n",
    "\n",
    "                db.to_disk(\"./train.spacy\") # save the docbin object\n",
    "\n",
    "            stream = os.popen('python3 -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy')\n",
    "            print(stream.read())\n",
    "            print(\"Total keywords: \", str(len(keywords)))\n",
    "            print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "            for element in myTweets:\n",
    "                findNewKeywords(element[0], keywords)\n",
    "\n",
    "            eval_model()\n",
    "            \n",
    "        \n",
    "\n",
    "        #for element in myTweets:\n",
    "            #findNewKeywords(element[0], keywords)\n",
    "\n",
    "        print(\"Total keywords: \", str(len(keywords)))\n",
    "        print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "        counter += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def information(data):\n",
    "    myData = {}\n",
    "    totalEnt=0\n",
    "    \n",
    "    for i in range(len(data[0])):\n",
    "        doc = nlp(preProcess(data[0][i]))\n",
    "        if len(doc.ents) == 0:\n",
    "            noEntity.append(preProcess(data[0][i]))\n",
    "            \n",
    "        #print(len(doc.ents))\n",
    "        if(len(doc.ents) == 4):\n",
    "            print(doc)\n",
    "            \n",
    "        for entity in doc.ents: \n",
    "        #print(entity.label_)\n",
    "            totalEnt+=1\n",
    "            if(entity.label_ == 'FOOD'):\n",
    "                if entity.text in myData:\n",
    "                    myData[entity.text] += 1\n",
    "                else:\n",
    "                    myData[entity.text] = 1\n",
    "                    \n",
    "    print(\"Number of entities found: \" + str(len(myData)))\n",
    "    print(totalEnt)\n",
    "    for i in sorted(myData, key = myData.get):\n",
    "        print(\"Entity: \" + i, \"Count: \" + str(myData[i]), \"Density: \" + str(format(myData[i]/totalEnt, '.2f')), end = \"\\n\")\n",
    "    \n",
    "    \n",
    "    return myData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(test_data['tweet'])):\n",
    "    test_data['tweet'][i] = preProcess(test_data['tweet'][i])\n",
    "\n",
    "\n",
    "\n",
    "y = test_data['food'].tolist()\n",
    "print(nlp.pipe_names)\n",
    "    \n",
    "def ent_recognize(text):\n",
    "    doc = nlp(text)\n",
    "    displacy.render(doc,style = \"ent\")\n",
    "    \n",
    "def predict(tweet):\n",
    "    doc = nlp(str(tweet))\n",
    "    if doc.ents:\n",
    "        displacy.render(doc,style = \"ent\")\n",
    "\n",
    "def returnPrediction(tweet):\n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    doc = nlp(str(tweet))\n",
    "    if doc.ents:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_predictions():\n",
    "    predictions = []\n",
    "    for tweet in test_data['tweet'].tolist():\n",
    "        predictions.append(returnPrediction(tweet))\n",
    "    return predictions\n",
    "    \n",
    "def eval_model():\n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    predictions = get_predictions()\n",
    "    print(metrics.confusion_matrix(y,predictions, labels = [1,0]))\n",
    "    print(metrics.classification_report(y,predictions, labels = [1,0]))\n",
    "    \n",
    "def show_tp():\n",
    "    counter = 0\n",
    "    tweets = test_data['tweet'].tolist()\n",
    "    predictions = get_predictions()\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 1 and y[i] == 1:\n",
    "            print(\"True positives:\", tweets[i], \"\\n\")\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "def show_tn():\n",
    "    counter = 0\n",
    "    predictions = get_predictions()\n",
    "    tweets = test_data['tweet'].tolist()\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 0 and y[i] == 0:\n",
    "            print(\"True Negative:\", tweets[i], \"\\n\")\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "def show_fn():\n",
    "    predictions = get_predictions()\n",
    "    tweets = test_data['tweet']\n",
    "    counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 0 and y[i] == 1:\n",
    "            print(\"False Negative:\", tweets[i], \"\\n\")\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "def show_fp():\n",
    "    predictions = get_predictions()\n",
    "    tweets = test_data['tweet'].tolist()\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 1 and y[i] == 0:\n",
    "            print(\"False Positive:\")\n",
    "            doc = nlp(str(tweets[i]))\n",
    "            if doc.ents:\n",
    "                displacy.render(doc,style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the function below to check individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chicken\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " is tasty</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ent_recognize(\"My chicken is tasty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the function below to check model performance on the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 44]\n",
      " [ 0 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        44\n",
      "           0       0.47      1.00      0.64        39\n",
      "\n",
      "    accuracy                           0.47        83\n",
      "   macro avg       0.23      0.50      0.32        83\n",
      "weighted avg       0.22      0.47      0.30        83\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brandon/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brandon/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the functions below to see TP, TN, FP, FN respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_tp()\n",
    "# show_tn()\n",
    "# show_fp()\n",
    "#show_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2716a7f28add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyTrainingdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0ce7d2b2f97b>\u001b[0m in \u001b[0;36minformation\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mnoEntity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             )\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     def update(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myTrainingdata = information(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets where no entities were found:  12716 \n",
      "\n",
      "<USERNAME> billie is butter?\n",
      "\"the anasa life coco smooth body butter is soooo amazing. i got the yuce scent, which is light and citrusy. the texture is so soft, and not too heavy or oily. it's absolutely perfect. if you love natural, quality body products, get this...\" - arielle b.\n",
      "dairy. all dairy products, like milk, butter, yogurt, and cheese, must come from a kosher animal.\n",
      "<USERNAME> bread , butter and sugar !!!\n",
      "i use body butter. but, yes after every bath/shower! it’s so important to moisturize! <URL>\n",
      "<USERNAME> billie is butter?\n",
      "as the brand &amp; content lead of <USERNAME>, buzzfeed's black culture vertical, <USERNAME> aims to \"push the culture forward.\"  \"life is so short, so while i'm here, i'm going to disrupt some things, shake the table, and make my presence known.\" <URL>\n",
      "<USERNAME> like. butter bread\n",
      "<USERNAME> <USERNAME> <USERNAME> love that video!!!!! so does peanut butter!!!!!!\n",
      "<USERNAME> <USERNAME> <USERNAME> a bit coin can buy over 3x as much milk, butter, gas, etc. as it could just a few years ago.  not so for the usd.  so the dollar is more stable.\n"
     ]
    }
   ],
   "source": [
    "#Tweets where no entities were found\n",
    "print(\"Number of Tweets where no entities were found: \", len(noEntity), '\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    print(noEntity[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total foodkeeper food names: 466\n",
      "\"genuine\" maple syrup\n",
      "aioli\n",
      "almond butter\n",
      "almond extract\n",
      "almond milk\n",
      "almond oil\n",
      "almonds\n",
      "amaranth\n",
      "anchovies\n",
      "apple cider\n",
      "apple cider vinegar\n",
      "apple juice\n",
      "apples\n",
      "applesauce\n",
      "apricots\n",
      "artichokes, whole\n",
      "arugula\n",
      "asparagus\n",
      "avocado oil\n",
      "avocados\n",
      "baby carrots\n",
      "bacon\n",
      "bacon bits\n",
      "bacon grease\n",
      "bagel\n",
      "bagged greens\n",
      "baking powder\n",
      "baking soda\n",
      "balsamic vinegar\n",
      "bamboo shoots\n",
      "bananas\n",
      "barbecue sauce\n",
      "barley\n",
      "base\n",
      "basil\n",
      "bean sprouts\n",
      "beans\n",
      "beans and peas\n",
      "beef\n",
      "beef broth/stock/consommé\n",
      "beets\n",
      "berries\n",
      "biscuit or pancake mix\n",
      "biscuits\n",
      "bison\n",
      "black bean sauce\n",
      "black pepper\n",
      "blueberries\n",
      "bok choy\n",
      "bratwurst\n",
      "bread\n",
      "breadcrumbs\n",
      "broccoli and broccoli raab (rapini)\n",
      "broth\n",
      "brussels sprouts\n",
      "buckwheat\n",
      "bulgur\n",
      "butter\n",
      "butter flavor\n",
      "buttermilk\n",
      "cabbage\n",
      "cajun seasoning blend\n",
      "cake, brownie, bread mixes\n",
      "canadian bacon\n",
      "canned chicken\n",
      "canned goods\n",
      "canola oil\n",
      "cantaloupe\n",
      "capers\n",
      "capon\n",
      "carrot juice\n",
      "carrots, parsnips\n",
      "cashew butter\n",
      "cashews\n",
      "casseroles\n",
      "cauliflower\n",
      "caviar\n",
      "celery\n",
      "celery root\n",
      "cereal\n",
      "cereal or granola bars\n",
      "cereal, dry mixes\n",
      "cheese\n",
      "cheese curds\n",
      "cheesecake\n",
      "cherimoya\n",
      "cherries\n",
      "cherry tomatoes\n",
      "chia seeds\n",
      "chicken\n",
      "chicken broth/stock/consommé\n",
      "chicken nuggets, patties\n",
      "chicken parts\n",
      "chicken salad\n",
      "chiffon pies\n",
      "chili powder\n",
      "chives\n",
      "chocolate\n",
      "chocolate hazlenut spread\n",
      "chocolate syrup\n",
      "chorizo\n",
      "chutney\n",
      "cilantro\n",
      "cinnamon\n",
      "cinnamon extract\n",
      "cinnamon rolls\n",
      "citrus fruit\n",
      "cocoa and cocoa mixes\n",
      "coconut\n",
      "coconut cream\n",
      "coconut flavor\n",
      "coconut flour\n",
      "coconut milk\n",
      "coconut oil\n",
      "coconut water\n",
      "coconuts\n",
      "coffee\n",
      "coffee creamer\n",
      "coleslaw\n",
      "commercial brand vacuum-packed dinners\n",
      "commercial bread products\n",
      "commercial cakes and muffins\n",
      "cooked fish\n",
      "cooked pasta\n",
      "cooked poultry dishes\n",
      "cooked rice\n",
      "cooked shellfish\n",
      "cookie dough\n",
      "cookies\n",
      "cooking wine\n",
      "corn on the cob\n",
      "corn syrup\n",
      "corned beef\n",
      "cornish hens\n",
      "cornmeal\n",
      "cornstarch\n",
      "cottage cheese\n",
      "crab legs\n",
      "crab meat\n",
      "crackers\n",
      "cranberries\n",
      "cranberry sauce\n",
      "cream\n",
      "cream cheese\n",
      "cream liquors\n",
      "cream pies\n",
      "cream sauces, milk solids\n",
      "croutons\n",
      "cucumbers\n",
      "cumin\n",
      "dairy filled eclairs\n",
      "dates\n",
      "diet powder mixes and drink mixes\n",
      "dinners\n",
      "dips\n",
      "dough\n",
      "doughnuts\n",
      "dry egg noodles\n",
      "dry gravy mixes\n",
      "dry stuffing mix\n",
      "duck fat\n",
      "duckling\n",
      "edamame\n",
      "egg dishes\n",
      "egg salad\n",
      "egg substitutes\n",
      "eggnog\n",
      "eggplant\n",
      "eggs\n",
      "farro\n",
      "fatty fish\n",
      "fish\n",
      "flavored or herb mixes\n",
      "flaxseed\n",
      "flour\n",
      "formula\n",
      "fresh clams, mussels, oysters\n",
      "fresh lobster tails\n",
      "fresh pasta\n",
      "fresh whole lobster\n",
      "fried chicken\n",
      "frosting or icing\n",
      "frozen entrees\n",
      "frozen potato products\n",
      "frozen pretzels\n",
      "fruit\n",
      "fruit cake\n",
      "fruit cocktail\n",
      "fruit juice in cartons, fruit drinks, punch\n",
      "fruit pies\n",
      "fruit, cut\n",
      "fruits\n",
      "fruits, dried\n",
      "frying oil\n",
      "garam masala\n",
      "garlic\n",
      "garlic powder\n",
      "gelatin\n",
      "ghee\n",
      "giblets\n",
      "ginger root\n",
      "goat\n",
      "goose\n",
      "graham cracker/animal cracker\n",
      "granola\n",
      "grapes\n",
      "grapeseed oil\n",
      "gravy\n",
      "greens\n",
      "grits\n",
      "ground turkey or chicken\n",
      "guacamole\n",
      "guava\n",
      "gummy (fruit) snacks\n",
      "ham\n",
      "ham salad\n",
      "hard liquors\n",
      "herbs\n",
      "herring\n",
      "hoisin sauce\n",
      "honey\n",
      "honeydew\n",
      "horseradish\n",
      "hot dogs\n",
      "hot peppers\n",
      "hot sauce\n",
      "hummus\n",
      "ice cream\n",
      "ice pops\n",
      "instant breakfast drinks\n",
      "jams, jellies, and preserves\n",
      "jars or pouches\n",
      "jerky\n",
      "jicama\n",
      "juice concentrates\n",
      "juice, boxes\n",
      "kale\n",
      "kefir\n",
      "ketchup, cocktail, or chili sauce\n",
      "kimchi\n",
      "kiwi fruit\n",
      "kohlrabi\n",
      "kugel\n",
      "kumquats\n",
      "lamb\n",
      "lean fish\n",
      "leeks\n",
      "leftovers\n",
      "lemon extract\n",
      "lemon juice\n",
      "lemongrass\n",
      "lentils\n",
      "lettuce\n",
      "lime juice\n",
      "liquid concentrate or ready-to-feed formula\n",
      "live clams, mussels, crab, and oysters\n",
      "lobster tails\n",
      "luncheon meat or poultry\n",
      "macadamias\n",
      "macaroons\n",
      "main dishes or meals\n",
      "margarine\n",
      "marinades\n",
      "marinated vegetables\n",
      "marshmallow crème\n",
      "marshmallows\n",
      "mayonnaise\n",
      "meat products\n",
      "meats\n",
      "melons\n",
      "milk\n",
      "millet\n",
      "mint\n",
      "miso\n",
      "molasses\n",
      "muffin\n",
      "mung bean\n",
      "mushrooms\n",
      "mustard\n",
      "nacho cheese\n",
      "nectar\n",
      "nut oils\n",
      "nutmeg\n",
      "nutrition supplement drinks\n",
      "nuts\n",
      "oats\n",
      "oils\n",
      "okra\n",
      "olives\n",
      "onion powder\n",
      "onions\n",
      "orange juice\n",
      "oregano\n",
      "oyster sauce\n",
      "pancakes, waffles\n",
      "papaya, mango, feijoa, passionfruit, casaha melon\n",
      "parsley\n",
      "pasta\n",
      "pasta salad\n",
      "pastrami\n",
      "pastries, danish\n",
      "pate\n",
      "peaches, nectarines, plums, pears, sapote\n",
      "peanut butter\n",
      "peanuts\n",
      "peas\n",
      "pecans\n",
      "pectin\n",
      "peppers\n",
      "pesto\n",
      "pheasant\n",
      "pickles\n",
      "pie crust\n",
      "pies\n",
      "pimento cheese\n",
      "pine nuts\n",
      "pineapple\n",
      "pistachios\n",
      "pitaya/dragon fruit\n",
      "pizza\n",
      "plantains\n",
      "polenta\n",
      "pomegranate\n",
      "popcorn\n",
      "pork\n",
      "pork rinds\n",
      "pork roll\n",
      "potato chips\n",
      "potato salad\n",
      "potatoes\n",
      "poultry pieces\n",
      "powdered milk\n",
      "pretzels\n",
      "prickly pear\n",
      "prosciutto\n",
      "pudding\n",
      "pudding mixes\n",
      "puff pastry\n",
      "pumpkin seeds\n",
      "pumpkins\n",
      "pure vanilla extract\n",
      "quail\n",
      "quark\n",
      "quiche\n",
      "quinoa\n",
      "rabbit\n",
      "radicchio\n",
      "radishes\n",
      "raspberries\n",
      "raw kabobs with vegetables\n",
      "re-hydrated textured soy protein\n",
      "ready-to-bake pie crust\n",
      "red wine\n",
      "refried beans\n",
      "relish\n",
      "retort pouches or boxes\n",
      "rhubarb\n",
      "rice\n",
      "rice milk\n",
      "ricotta\n",
      "roasted nuts (peanuts, cashews, almonds)\n",
      "roasted red peppers\n",
      "rosemary\n",
      "rotisserie chicken\n",
      "rutabagas\n",
      "rye\n",
      "salad dressing\n",
      "salad dressings\n",
      "salami\n",
      "salsa\n",
      "salt\n",
      "sauce mixes\n",
      "sausage\n",
      "sausages\n",
      "scallops\n",
      "seafood\n",
      "seafood salads\n",
      "seasoning blends\n",
      "sesame oil\n",
      "sesame seeds\n",
      "sherbet, sorbet\n",
      "shortening\n",
      "shrimp, crayfish\n",
      "shrimp, shellfish\n",
      "shucked clams, mussels, and oysters\n",
      "soda\n",
      "sorghum\n",
      "soup mixes\n",
      "soup, stews\n",
      "sour cream\n",
      "soy crumbles and hot dogs\n",
      "soy flour\n",
      "soy meat substitutes\n",
      "soy milk\n",
      "soy or rice beverage\n",
      "soy sauce or teriyaki sauce\n",
      "spaghetti sauce\n",
      "spaghetti squash\n",
      "spelt\n",
      "spice/spices\n",
      "squash\n",
      "squid\n",
      "star fruit\n",
      "strawberries\n",
      "string cheese\n",
      "stuffed, raw chicken breasts\n",
      "stuffed, raw pork chops\n",
      "sugar\n",
      "sugar substitutes\n",
      "sun dried tomatoes\n",
      "sunflower oil\n",
      "sunflower seeds\n",
      "surimi seafood\n",
      "swiss chard\n",
      "syrup\n",
      "tahini\n",
      "tamarind\n",
      "tamarind paste\n",
      "tapenade\n",
      "tapiocas\n",
      "taro\n",
      "tea\n",
      "teff\n",
      "tempeh\n",
      "textured soy protein\n",
      "thai red curry paste\n",
      "thyme\n",
      "toaster pastries\n",
      "tofu\n",
      "tomato paste\n",
      "tomato sauce\n",
      "tomatoes\n",
      "tortillas\n",
      "tube cans\n",
      "tuna\n",
      "turducken\n",
      "turkey\n",
      "turkey bacon\n",
      "turkey parts\n",
      "turnips\n",
      "variety meats\n",
      "veal\n",
      "vegan cheddar cheese\n",
      "vegetable juice\n",
      "vegetable oil sprays\n",
      "vegetable soup\n",
      "vegetable stock/broth\n",
      "vegetables\n",
      "venison\n",
      "vinegar\n",
      "walnuts\n",
      "water\n",
      "watermelon\n",
      "whipped cream\n",
      "whipped topping\n",
      "white wine\n",
      "whole wheat bread\n",
      "whole wheat flour\n",
      "worcestershire sauce\n",
      "yams/sweet potatoes\n",
      "yeast\n",
      "yogurt\n",
      "yuca/cassava\n",
      "yuzu\n",
      "yuzu juice\n",
      "zucchini\n"
     ]
    }
   ],
   "source": [
    "foodkeeper = foodKeeperInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data entities:  409\n",
      "Foodkeeper entities:  466\n",
      "Union between foodkeeper and training data: 223\n",
      "Unique training data entities:  186\n",
      "Unique foodkeeper data entities:  243 \n",
      "\n",
      "~~~~~~~~~~~~Only Training~~~~~~~~~~~~\n",
      "0.15\n",
      "2.they\n",
      "4.99\n",
      "699,615,065,551\n",
      "abstract\n",
      "acai\n",
      "acerola\n",
      "activation\n",
      "adored\n",
      "agitating\n",
      "ahora\n",
      "alejandro\n",
      "algae\n",
      "alvarrez\n",
      "approached\n",
      "aslan\n",
      "athletes\n",
      "attracted\n",
      "azul\n",
      "bffs\n",
      "blues\n",
      "bootcamps\n",
      "braves\n",
      "brazils\n",
      "bulborb\n",
      "cafe\n",
      "calamagrostis\n",
      "cameron\n",
      "capricorn\n",
      "capsaicin\n",
      "carafe\n",
      "carbseedling\n",
      "cested\n",
      "chickenbreast\n",
      "chunibyo\n",
      "citeh\n",
      "clashes\n",
      "coconutsㅋㅋㅋ\n",
      "collapse\n",
      "commodified\n",
      "corn‼️\n",
      "coupled\n",
      "creamery\n",
      "cuddling\n",
      "curbside\n",
      "d175\n",
      "dalmunach\n",
      "dani\n",
      "decadent!⁣\n",
      "delusional\n",
      "derwin\n",
      "dinnerly\n",
      "doggos\n",
      "drains\n",
      "dram\n",
      "dranks\n",
      "driveable\n",
      "enrolled\n",
      "erika\n",
      "european\n",
      "exporters\n",
      "extent\n",
      "firewild\n",
      "frickin\n",
      "fuckn\n",
      "gals\n",
      "gasc\n",
      "glenbrook\n",
      "goddddddd\n",
      "goldening\n",
      "grained\n",
      "grann\n",
      "griswald\n",
      "gulf\n",
      "hachinohe\n",
      "hamin\n",
      "hank\n",
      "hauck\n",
      "helga\n",
      "hibby\n",
      "hocking\n",
      "howd\n",
      "iffco\n",
      "indians\n",
      "insects\n",
      "karedok\n",
      "kiddush\n",
      "kiita\n",
      "kirkby\n",
      "knabe\n",
      "krogan\n",
      "latino\n",
      "libya\n",
      "lindsay\n",
      "lure\n",
      "mach\n",
      "malnourished\n",
      "marathi\n",
      "matos\n",
      "mcfarlane\n",
      "melville\n",
      "mercadona\n",
      "miyuki\n",
      "montgomery\n",
      "monuments\n",
      "morphological\n",
      "mothballs\n",
      "murderous\n",
      "murgor\n",
      "nairobi\n",
      "nazis\n",
      "nebraska\n",
      "nicca\n",
      "nintendo\n",
      "odor\n",
      "orridge\n",
      "outsource\n",
      "overlook\n",
      "overtake\n",
      "parlour\n",
      "patreon\n",
      "patron\n",
      "patterns\n",
      "petite\n",
      "pharmacist\n",
      "plateful\n",
      "plead\n",
      "pleeeease\n",
      "polishes\n",
      "pretended\n",
      "psychopaths\n",
      "quests\n",
      "radiance\n",
      "ramsey\n",
      "ras\n",
      "reconstituted\n",
      "relleno\n",
      "remastered\n",
      "resume\n",
      "risottos\n",
      "rockpalast\n",
      "ronni\n",
      "roobee\n",
      "saag\n",
      "sally\n",
      "screening\n",
      "sculpt\n",
      "shadiyon\n",
      "sheikh\n",
      "shelters\n",
      "showcases\n",
      "skinning\n",
      "skippy\n",
      "slammed\n",
      "snort\n",
      "snub\n",
      "solitude\n",
      "stanton\n",
      "striker\n",
      "stutzle\n",
      "subdued\n",
      "svelte\n",
      "syahleka\n",
      "throwback\n",
      "toledo\n",
      "tonnes\n",
      "torah\n",
      "tournament\n",
      "trimmings\n",
      "trop\n",
      "twewy\n",
      "ummmmmm\n",
      "undead\n",
      "unexpected\n",
      "urged\n",
      "vardy\n",
      "vibrate\n",
      "victoria\n",
      "wales\n",
      "welby\n",
      "whitewater\n",
      "wholly\n",
      "woolfardisworthy\n",
      "فوغا\n",
      "↪\n",
      "ㅋㅋㅋㅋmother\n",
      "~~~~~~~~~~~~Only Foodkeeper~~~~~~~~~~~~\n",
      "\"genuine\" maple syrup\n",
      "almond butter\n",
      "almond extract\n",
      "almond milk\n",
      "almond oil\n",
      "almonds\n",
      "apple cider\n",
      "apple cider vinegar\n",
      "apple juice\n",
      "artichokes, whole\n",
      "avocado oil\n",
      "baby carrots\n",
      "bacon bits\n",
      "bacon grease\n",
      "bagged greens\n",
      "baking powder\n",
      "baking soda\n",
      "balsamic vinegar\n",
      "bamboo shoots\n",
      "barbecue sauce\n",
      "bean sprouts\n",
      "beans and peas\n",
      "beef broth/stock/consommé\n",
      "biscuit or pancake mix\n",
      "black bean sauce\n",
      "black pepper\n",
      "bok choy\n",
      "broccoli and broccoli raab (rapini)\n",
      "brussels sprouts\n",
      "butter flavor\n",
      "cajun seasoning blend\n",
      "cake, brownie, bread mixes\n",
      "canadian bacon\n",
      "canned chicken\n",
      "canned goods\n",
      "canola oil\n",
      "carrot juice\n",
      "carrots, parsnips\n",
      "cashew butter\n",
      "celery root\n",
      "cereal or granola bars\n",
      "cereal, dry mixes\n",
      "cheese curds\n",
      "cherry tomatoes\n",
      "chia seeds\n",
      "chicken broth/stock/consommé\n",
      "chicken nuggets, patties\n",
      "chicken parts\n",
      "chicken salad\n",
      "chiffon pies\n",
      "chili powder\n",
      "chocolate hazlenut spread\n",
      "chocolate syrup\n",
      "cinnamon extract\n",
      "cinnamon rolls\n",
      "citrus fruit\n",
      "cocoa and cocoa mixes\n",
      "coconut cream\n",
      "coconut flavor\n",
      "coconut flour\n",
      "coconut milk\n",
      "coconut oil\n",
      "coconut water\n",
      "coffee creamer\n",
      "commercial brand vacuum-packed dinners\n",
      "commercial bread products\n",
      "commercial cakes and muffins\n",
      "cooked fish\n",
      "cooked pasta\n",
      "cooked poultry dishes\n",
      "cooked rice\n",
      "cooked shellfish\n",
      "cookie dough\n",
      "cooking wine\n",
      "corn on the cob\n",
      "corn syrup\n",
      "corned beef\n",
      "cornish hens\n",
      "cottage cheese\n",
      "crab legs\n",
      "crab meat\n",
      "cranberry sauce\n",
      "cream cheese\n",
      "cream liquors\n",
      "cream pies\n",
      "cream sauces, milk solids\n",
      "dairy filled eclairs\n",
      "diet powder mixes and drink mixes\n",
      "dry egg noodles\n",
      "dry gravy mixes\n",
      "dry stuffing mix\n",
      "duck fat\n",
      "egg dishes\n",
      "egg salad\n",
      "egg substitutes\n",
      "fatty fish\n",
      "flavored or herb mixes\n",
      "fresh clams, mussels, oysters\n",
      "fresh lobster tails\n",
      "fresh pasta\n",
      "fresh whole lobster\n",
      "fried chicken\n",
      "frosting or icing\n",
      "frozen entrees\n",
      "frozen potato products\n",
      "frozen pretzels\n",
      "fruit cake\n",
      "fruit cocktail\n",
      "fruit juice in cartons, fruit drinks, punch\n",
      "fruit pies\n",
      "fruit, cut\n",
      "fruits, dried\n",
      "frying oil\n",
      "garam masala\n",
      "garlic powder\n",
      "ginger root\n",
      "graham cracker/animal cracker\n",
      "grapeseed oil\n",
      "ground turkey or chicken\n",
      "gummy (fruit) snacks\n",
      "ham salad\n",
      "hard liquors\n",
      "hoisin sauce\n",
      "hot dogs\n",
      "hot peppers\n",
      "hot sauce\n",
      "ice cream\n",
      "ice pops\n",
      "instant breakfast drinks\n",
      "jams, jellies, and preserves\n",
      "jars or pouches\n",
      "juice concentrates\n",
      "juice, boxes\n",
      "ketchup, cocktail, or chili sauce\n",
      "kiwi fruit\n",
      "lean fish\n",
      "lemon extract\n",
      "lemon juice\n",
      "lime juice\n",
      "liquid concentrate or ready-to-feed formula\n",
      "live clams, mussels, crab, and oysters\n",
      "lobster tails\n",
      "luncheon meat or poultry\n",
      "main dishes or meals\n",
      "marinated vegetables\n",
      "marshmallow crème\n",
      "meat products\n",
      "mung bean\n",
      "nacho cheese\n",
      "nut oils\n",
      "nutrition supplement drinks\n",
      "onion powder\n",
      "orange juice\n",
      "oyster sauce\n",
      "pancakes, waffles\n",
      "papaya, mango, feijoa, passionfruit, casaha melon\n",
      "pasta salad\n",
      "pastries, danish\n",
      "peaches, nectarines, plums, pears, sapote\n",
      "peanut butter\n",
      "pie crust\n",
      "pimento cheese\n",
      "pine nuts\n",
      "pitaya/dragon fruit\n",
      "pork rinds\n",
      "pork roll\n",
      "potato chips\n",
      "potato salad\n",
      "poultry pieces\n",
      "powdered milk\n",
      "prickly pear\n",
      "pudding mixes\n",
      "puff pastry\n",
      "pumpkin seeds\n",
      "pure vanilla extract\n",
      "raw kabobs with vegetables\n",
      "re-hydrated textured soy protein\n",
      "ready-to-bake pie crust\n",
      "red wine\n",
      "refried beans\n",
      "retort pouches or boxes\n",
      "rice milk\n",
      "roasted nuts (peanuts, cashews, almonds)\n",
      "roasted red peppers\n",
      "rotisserie chicken\n",
      "salad dressing\n",
      "salad dressings\n",
      "salami\n",
      "sauce mixes\n",
      "seafood salads\n",
      "seasoning blends\n",
      "sesame oil\n",
      "sesame seeds\n",
      "sherbet, sorbet\n",
      "shrimp, crayfish\n",
      "shrimp, shellfish\n",
      "shucked clams, mussels, and oysters\n",
      "soup mixes\n",
      "soup, stews\n",
      "sour cream\n",
      "soy crumbles and hot dogs\n",
      "soy flour\n",
      "soy meat substitutes\n",
      "soy milk\n",
      "soy or rice beverage\n",
      "soy sauce or teriyaki sauce\n",
      "spaghetti sauce\n",
      "spaghetti squash\n",
      "spice/spices\n",
      "star fruit\n",
      "string cheese\n",
      "stuffed, raw chicken breasts\n",
      "stuffed, raw pork chops\n",
      "sugar substitutes\n",
      "sun dried tomatoes\n",
      "sunflower oil\n",
      "sunflower seeds\n",
      "surimi seafood\n",
      "swiss chard\n",
      "tamarind paste\n",
      "textured soy protein\n",
      "thai red curry paste\n",
      "toaster pastries\n",
      "tomato paste\n",
      "tomato sauce\n",
      "tube cans\n",
      "turkey bacon\n",
      "turkey parts\n",
      "variety meats\n",
      "vegan cheddar cheese\n",
      "vegetable juice\n",
      "vegetable oil sprays\n",
      "vegetable soup\n",
      "vegetable stock/broth\n",
      "whipped cream\n",
      "whipped topping\n",
      "white wine\n",
      "whole wheat bread\n",
      "whole wheat flour\n",
      "worcestershire sauce\n",
      "yams/sweet potatoes\n",
      "yuca/cassava\n",
      "yuzu juice\n"
     ]
    }
   ],
   "source": [
    "##     Information about the different datasets\n",
    "x = list(myTrainingdata)\n",
    "\n",
    "print(\"Training data entities: \",len(x))\n",
    "print(\"Foodkeeper entities: \", len(foodkeeper))\n",
    "\n",
    "\n",
    "diff = list(set(x) & set(foodkeeper))\n",
    "print(\"Union between foodkeeper and training data: \" + str(len(diff)))\n",
    "#for elem in sorted(diff):\n",
    "    #print(elem)\n",
    "\n",
    "onlyTraining = list(set(x) - set(foodkeeper))\n",
    "onlyFoodkeeper = list(set(foodkeeper) - set(x))\n",
    "\n",
    "        \n",
    "print(\"Unique training data entities: \", len(onlyTraining))\n",
    "print(\"Unique foodkeeper data entities: \",len(onlyFoodkeeper), '\\n')\n",
    "\n",
    "print(\"~~~~~~~~~~~~Only Training~~~~~~~~~~~~\")\n",
    "for element in sorted(onlyTraining):\n",
    "    print(element)\n",
    "\n",
    "print(\"~~~~~~~~~~~~Only Foodkeeper~~~~~~~~~~~~\")\n",
    "for element in sorted(onlyFoodkeeper):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~0~~~~~~~~~~~~~~~~~\n",
      "Total keywords:  10\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'cream', 'fruit', 'rice', 'water', 'garlic', 'bread'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~1~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 232.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     32.00    0.00    0.00    0.00    0.00\n",
      "  5     200         36.92    822.22   99.84   99.67  100.00    1.00\n",
      " 12     400         43.99     23.78  100.00  100.00  100.00    1.00\n",
      " 22     600         33.45     10.12  100.00  100.00  100.00    1.00\n",
      " 33     800         21.53      5.21  100.00  100.00  100.00    1.00\n",
      " 47    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 65    1200         18.04      5.95  100.00  100.00  100.00    1.00\n",
      " 87    1400         59.45     15.18  100.00  100.00  100.00    1.00\n",
      "113    1600         14.77      4.50  100.00  100.00  100.00    1.00\n",
      "146    1800         75.13     13.04  100.00  100.00  100.00    1.00\n",
      "186    2000         43.99      7.76  100.00  100.00  100.00    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n",
      "\n",
      "Total keywords:  10\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'cream', 'fruit', 'rice', 'water', 'garlic', 'bread'] \n",
      "\n",
      "\n",
      "[[18 26]\n",
      " [ 4 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.41      0.55        44\n",
      "           0       0.57      0.90      0.70        39\n",
      "\n",
      "    accuracy                           0.64        83\n",
      "   macro avg       0.70      0.65      0.62        83\n",
      "weighted avg       0.70      0.64      0.62        83\n",
      "\n",
      "Total keywords:  82\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'cream', 'fruit', 'rice', 'water', 'garlic', 'bread', 'squash', 'soda', 'flour', 'buttermilk', 'apples', 'potatoes', 'onions', 'fish', 'turkey', 'beans', 'bagel', 'yogurt', 'pretzels', 'celery', 'crackers', 'dips', 'meats', 'tuna', 'eggs', 'bacon', 'pasta', 'pizza', 'cheesecake', 'cinnamon', 'muffin', 'chocolate', 'pies', 'pickles', 'cornstarch', 'broth', 'salt', 'parsley', 'grits', 'chives', 'tomatoes', 'cumin', 'pork', 'beef', 'greens', 'peas', 'okra', 'cabbage', 'basil', 'lentils', 'nuts', 'fruits', 'vegetables', 'pesto', 'vinegar', 'honey', 'peanuts', 'peppers', 'chutney', 'sausage', 'coffee', 'goat', 'cereal', 'grapes', 'cashews', 'tea', 'pudding', 'granola', 'coconut', 'mayonnaise', 'raspberries', 'strawberries', 'berries', 'coleslaw', 'oregano', 'cauliflower', 'olives', 'salsa'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~2~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 32/426 [00:00<00:01, 312.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 87/426 [00:00<00:02, 133.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 117/426 [00:01<00:03, 89.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 208/426 [00:02<00:03, 54.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 290/426 [00:04<00:02, 47.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 315/426 [00:05<00:02, 42.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 388/426 [00:07<00:01, 31.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 415/426 [00:08<00:00, 32.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 426/426 [00:08<00:00, 49.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    }
   ],
   "source": [
    "trainModel(training_data)\n",
    "#print(keywords)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My friend likes to drink milk and eat \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    fish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chicken\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My friend is a chicken</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">A chicken is a wild animal.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chicken\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " is a wild animal.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(MODEL_PATH)\n",
    "ent_recognize(\"My friend likes to drink milk and eat fish and chicken\")\n",
    "ent_recognize(\"My friend is a chicken\")\n",
    "ent_recognize(\"A chicken is a wild animal.\")\n",
    "ent_recognize(\"The chicken is a wild animal.\")\n",
    "\n",
    "#rankTweet(\"sugar\")\n",
    "#eval_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_model()\n",
    "# show_tp()\n",
    "# show_tn()\n",
    "# show_fp()\n",
    "#show_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankTweet(\"Hello chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chicken', 'milk']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
